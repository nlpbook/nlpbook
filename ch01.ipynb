{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[ch01]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think your computer can do?\n",
    "\n",
    "Show you emails? Edit some files? Spin up an excel sheet maybe?\n",
    "\n",
    "But what if I told you your computer could read?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996862411499023}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier('I am reading the greatest NLP book ever!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Welcome to the latest news'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator = pipeline(\"text-generation\")\n",
    "text_generator(\"Welcome to the\", max_length=5, do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and, most impressively, understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9869255423545837,\n",
       " 'start': 1,\n",
       " 'end': 28,\n",
       " 'answer': 'Natural language processing'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics,\n",
    "computer science, and artificial intelligence concerned with the\n",
    "interactions between computers and human language, in particular\n",
    "how to program computers to process and analyze large amounts of\n",
    "natural language data. The result is a computer capable of\n",
    "\"understanding\" the contents of documents, including the contextual\n",
    "nuances of the language within them. The technology can then accurately\n",
    "extract information and insights contained in the documents as well\n",
    "as categorize and organize the documents themselves.\n",
    "\"\"\"\n",
    "nlp(question=\"What is NLP?\", context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was once the fantasy of a distant future is not only here but is accessible to anyone with a computer and an internet connection. The ability to understand and communicate in natural language, one of the most valuable assets that humanity has developed over the course of our existence, is now practical to do on machines.\n",
    "\n",
    "\"Of course!\" you proclaim. \"Technology always gets better, and we've had speech recognition and google translate for ages!\"\n",
    "\n",
    "But even just five years ago, \"NLP\" was something better suited to TechCrunch articles than actual production codebases. In the last three years, we've seen an exponential growth in progress in the field, models being deployed in production *today* are vastly superior to the most obscure research leaderboards from the days past.\n",
    "\n",
    "But we're gettign ahead of ourselves. Before we delve deeper, let’s start with a high level overview of the field. Once we cover the basics, we will introduce more advanced topics. Our goal is to help you build intuition and experience working with NLP, chapter by chapter, so that by the end of the book, you'll be able to build _real applications_ that add _real value_ to the world.\n",
    "\n",
    "In the first half of this chapter, we will define NLP, explore some commercial applications of the technology, and walk through how the field has evolved since its origins in the 1950s.\n",
    "\n",
    "In the second half of the chapter, we will introduce a very performant NLP library that is popular in enterprise and use it to perform basic NLP tasks. While these tasks _are_ elementary, when combined together, they allow computers to process and analyze natural language data in complex ways that make amazing commercial applications such as chatbots and voicebots possible.\n",
    "\n",
    "In some ways, the process of machines learning how to process language is similar to how toddlers begin to learn language by mumbling and fumbling over words only to later speak in full sentences and paragraphs. As we move through the book, we will build on the basic NLP tasks covered in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s begin by defining what natural language processing is. Here is how NLP is commonly defined.\n",
    "\n",
    "```asciidoc\n",
    "[quote, Wikipedia]\n",
    "____\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
    "____\n",
    "```\n",
    "\n",
    "Let’s unpack this definition. When we say \"natural language,\" we mean \"human language\" as opposed to programming languages. Natural language refers to not only textual data but also to speech and audio data.\n",
    "\n",
    "Great, but so what if computers can now work with large amounts of text, speech, and audio data? Why is this so important?\n",
    "\n",
    "Imagine for a second the world without language. How would we communicate via text or speech? How would we read books, listen to music, or comprehend movies and TV shows? Life as we know it would cease to exist; we would be stuck in caveman days, able to process information visually but unable to share our knowledge with each other or communicate in any meaningful way. footnote:[One of the major leaps in human history is the formation of a human (aka \"natural\") language, which allowed humans to communicate with one another, form groups, and operate as collective units of people instead of as solo individuals.]\n",
    "\n",
    "Likewise, if machines can work with only numerical and visual data but cannot process natural language, machines would be limited in the number and variety of applications they would have in the real world. Without the ability to handle natural language, machines will never be able to approach general artificial intelligence or anything that resembles human intelligence today.\n",
    "\n",
    "Fortunately, machines can now finally process natural language data reasonably well. Let’s explore what commercial applications are possible because of this relatively newfound ability of computers to work with natural language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the advances in NLP, machines are able to handle a broad array of natural language tasks, at least in a rudimentary way. Here are some common applications of NLP today.\n",
    "\n",
    "- **Machine translation:** Machine translation is the process of using machines to translate from one language to another without any human intervention. By far the most popular example of this is Google Translate, which supports over 100 languages and serves over 500 million people daily. When it was first launched in 2006, the performance of Google Translate was notably worse than what it is today. Performance today is fast approaching human expert-level.footnote:[For more, read The New York Times article from 2016 on https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html[Google's neural machine translation].]\n",
    "\n",
    "- **Speech recognition:** It may sound shocking, but voice recognition technology has been around for over 50 years. None of the voice recognition software had good performance or gone mainstream until very recently, driven by the rise of deep learning. Today, Amazon Alexa, Apple Siri, Google Assistant, Microsoft Cortana, digital voice assistants in your car, and other software are now able to recognize speech with such a high level of accuracy that the software is able to process the information real-time and answer in a mostly reasonable way. Even as little as fifteen years ago, the ability of such machines to recognize speech and respond in a coherent manner was abysmal.\n",
    "\n",
    "- **Question answering:** For these digital assistants to deliver a delightful experience to humans asking questions, speech recognition is only the first half of the job. The software needs to (a) recognize the speech (b) given the speech recognized, retrieve an appropriate response. This second half is known as question answering (QA).\n",
    "\n",
    "- **Text summarization:** One of the most common tasks humans do everyday, especially in white collar desk jobs, is read long form documents and summarize the contents. Machines are now able to perform this summarization, creating a shorter summary of a longer text document. Text summarization reduces the reading time for humans. Humans that analyze lots of text daily (i.e., lawyers, paralegals, business analysts, students, etc.) are able to shift through the machine-generated short summaries of long form documents and then, based on the summaries, choose the relevant documents to read more thoroughly.\n",
    "\n",
    "- **Chatbots:** If you have spent some time perusing websites recently, you may have realized that more and more sites now have a chatbot that automatically chimes in to engage the human user. The chatbot usually greets the human in a friendly, nonthreatening manner and then asks the user questions to gauge the purpose and intent of the visit to the site. The chatbot then tries to automatically answer the questions without any human intervention. Such chatbots are now automating digital customer engagement.\n",
    "\n",
    "- **Text-to-speech and speech-to-text:** Software is now able to convert text to high-fidelity audio very easily. For example, Google Cloud Text-to-Speech is able to convert text into human-like speech in more than 180 voices across over 30 languages. Likewise, Google's Cloud Speech-to-Text is able to convert audio to text for over 120 languages, delivering a truly global offering.\n",
    "\n",
    "- **Voicebots:** Ten years ago, automated voice agents were clunky. Unless humans responded in a fairly constrained manner (e.g., with yes or no type responses), the voice agents on the phone could not process the information. Now, AI voicebots like those provided by Voiq are able to help augment and automate calls for sales, marketing, and customer success teams.\n",
    "\n",
    "- **Text and audio generation:** Years ago, text generation relied on templates and rules-based systems. This limited the scope of application. Now, software is able to generate text and audio using machine learning, broadening the scope of application considerably. For example, Gmail is now able to suggest entire sentences based on previous sentences you've drafted, and it's able to do this on the fly as you type. While natural language generation is best at short blurbs of text (partial sentences), soon such systems may be able to produce reasonably good long form content. A popular commerical application of natural language generation is data-to-text software, which generates textual summaries of databases and data sets. Data-to-text software includes data analysis as well as text generation. Firms in this space include Narrative Science and Automated Insights.\n",
    "\n",
    "- **Sentiment analysis:** With the explosion of social media content, there is an ever-growing need to automate customer sentiment analysis, dissecting tweets, posts, and comments for sentiment such as positive vs. negative vs. neutral or angry vs. sad vs. happy. Such software is also known as emotion AI. \n",
    "\n",
    "- **Information extraction:** One major challenge in NLP is creating structured data from unstructured and/or semi-structured documents. For example, named entity recognition software is able to extract people, organizations, locations, dates, and currencies from long form texts such as mainstream news. Information extraction also involves relationship extraction, identifying the relations between entities, if any.\n",
    "\n",
    "The number of NLP applications in enterprise has exploded over the past decade, ranging from speech recognition and question and answering to voicebots and chatbots that are able to generate natural language on their own. This is quite astounding given where the field was a few decades ago.\n",
    "\n",
    "To put the current progress in NLP into perspective, let’s walk through how NLP has progressed, starting from its origins in 1950."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of natural language processing has been around for nearly 70 years. Perhaps most famously, Alan Turing laid the foundation for the field by developing the Turing test in 1950. The Turing test is a test of a machine's ability to demonstrate intelligence that is indistinguishable from that of a human. For the machine to pass the Turing test, the machine must generate human-like responses such that a human evaluator would not be able to tell whether the responses were generated by a human or a machine (i.e., the machine's responses are of human quality).footnote:[For more on the [Turing test](https://en.wikipedia.org/wiki/Turing_test), please refer to the Wikipedia article.]\n",
    "\n",
    "The Turing test launched significant debate in the then-nascent artificial intelligence field and spurred researchers to develop natural langugage processing models that would serve as building blocks for a machine that someday may pass the Turing test, a search that continues to this day.\n",
    "\n",
    "Like the broader field of artificial intellgience, NLP has had many booms and busts, lurching from hype cycles to AI winters. In 1954, Georgetown University and IBM successfully built a system that could automatically translate more than sixty Russian sentences to English. At the time, researchers at Georgetown University thought machine translation would be a solved problem within three to five years. The success in the U.S. also spurred the Soviet Union to launch similar efforts. The Georgetown-IBM success coupled with the Cold War mentality led to increased funding for NLP in these early years.\n",
    "\n",
    "However, by 1966, progress had stalled, and the Automatic Language Processing Advisory Committee (known as ALPAC) - a U.S. government agency set up to evaluate the progress in computational linguistics - released a sobering report. The report stated that machine translation was more expensive, less accurate, and slower than human translation and unlikely to reach human-level performance in the near future. The report led to a reduction in funding for machine translation research. Following the report, research in the field nearly died for almost a decade.\n",
    "\n",
    "Despite these setbacks, the field of NLP re-emerged in the 1970s. By the 1980s, computational power had increased significantly and costs had come down sufficiently, opening up the field to many more researchers around the world.\n",
    "\n",
    "In the late 1980s, NLP rose in prominence again with the release of the first statistical machine translation systems, led by researchers at IBM's Thomas J. Watson Research Center. Prior to the rise of statistical machine translation, machine translation relied on human hand-crafted rules for language. These systems were called rules-based machine translation. The rules would help correct and control mistakes that the machine translation systems would typically make, but crafting such rules was a laborious and painstaking process. The machine translation systems were also brittle as a result; if the machine translation systems encountered edge case scenarios for which rules had not been developed, the machine translation systems would fail, sometimes egregiously.\n",
    "\n",
    "Statistical machine translation helped reduce the need for human hand-crafted rules. Statistical machine translation relied much more heavily on learning from data. Using a bilingual corpus with parallel texts as data (i.e., two texts that are identical except for the language they are written in), such systems would carve sentences into small subsets and translate the subsets segment-by-segment from the source language to the target language. The more data (i.e., bilingual text corpora) the system would have, the better the translation.\n",
    "\n",
    "Statistical machine translation would remain the most widely studied and used machine translation method until the rise of neural machine translation in the mid-2010s.\n",
    "\n",
    "By the 1990s, such successes led researchers to expand beyond text into speech recognition. Speech recognition, like machine translation, had been around since the early 1950s, spurred by early successes by the likes of Bell Labs and IBM. But speech recognition systems had severe limitations. In the 1960s, for example, such systems could take voice commands for playing chess but not do much else.\n",
    "\n",
    "By the mid-1980s, IBM applied a statistical approach to speech recognition and launched a voice activated typewriter called Tangora, which could handle a 20,000-word vocabulary.\n",
    "\n",
    "DARPA, Bell Labs, and Carnegie Mellon University also had similar successes by the late 1980s. Speech recognition software systems by then had larger vocabularies than the average human and could handle continuous speech recognition, a milestone in the history of speech recognition.\n",
    "\n",
    "In the 1990s, several researchers in the space left research labs and universities to work in industry, which led to more commercial applications of speech recognition and machine translation.\n",
    "\n",
    "Today's NLP heavyweights such as Google hired their first speech recognition employees in 2007. The U.S. government also got involved then; the National Security Agency began tagging large volumes of recorded conversations for specific keywords, facilitating the search process for NSA analysts.\n",
    "\n",
    "By the early 2010s, NLP researchers, both in academia and industry, began experimenting with deep neural networks for NLP tasks. Early deep learning-led successes came from a deep learning method called long short-term memory (LSTM). In 2015, Google used such a method to revamp Google Voice.\n",
    "\n",
    "Deep learning methods led to dramatic performance improvements in NLP tasks, spurring more dollars into the space. These successes have led to a much deeper integration of NLP software in our everyday lives.\n",
    "\n",
    "For example, cars in the early 2010s had voice recognition software that could handle a limited set of voice commands; now cars have tech that could handle a much broader set of natural language commands, inferring context and intent much more clearly.\n",
    "\n",
    "Looking back today, progress in NLP was slow but steady, moving from rules-based systems in the early days to statistical machine translation by the 1980s and to neural network-based systems by the 2010s. While academic research in the space has been fierce for quite some time, NLP has become a mainstream topic only recently. Let's examine the main inflection points over the past several years that have helped NLP become one of the hottest topics in AI today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inflection Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP and computer vision are both sub-fields of artificial intelligence, but computer vision has had more successful commerical successes to date. Computer vision had its inflection point in 2012 (the so-called \"ImageNet\" moment) when a deep learning-based solution called AlexNet decimated the previous error rate of computer vision models. \n",
    "\n",
    "In the years since 2012, computer vision has powered applications such as auto-tagging of photos and videos, self-driving cars, cashier-less stores, facial recognition-powered authentication of devices, radiology diagnoses, and more.\n",
    "\n",
    "NLP has been a relatively late bloomer by comparison. NLP made waves from 2014 onwards with the release of Amazon Alexa, a revamped Apple Siri, Google Assistant, and Microsoft Cortana. Google also launched a much improved version of Google Translate in 2016, and now chatbots and voicebots are much more commonplace.\n",
    "\n",
    "That being said, it wasn't until 2018 that NLP had its very own ImageNet moment with the release of large pretrained language models trained using the Transformer architecture; the most notable of these was Google's BERT, which was launched in November 2018.\n",
    "\n",
    "In 2019, generative models such as OpenAI's GPT-2 made splashes, generating new content based on previous content on the fly, a previously insurmountable feat. In 2020, OpenAI released an even larger and more impressive version called GPT-3, building on its previous successes.\n",
    "\n",
    "Heading into 2021 and beyond, NLP is now no longer an experimental sub-field of AI. Along with computer vision, NLP is now poised to have many broad based applications in the enterprise. With this book, we hope to share some of concepts and tools that will help you build some of these applications at your company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Final Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not one single approach to solving NLP tasks. The three dominant approaches today are rule-based, traditional machine learning (statistical-based), and neural network-based.\n",
    "\n",
    "Let's explore each approach.\n",
    "\n",
    "- **Rule-based NLP:** Traditional NLP software relies heavily on human-crafted rules of languages; domain experts, typically linguists, curate these rules. You can think of these rules as regular expression or pattern-matching. Rule-based NLP perform well in narrowly scoped out use cases but typically do not generalize well. More and more rules are necessary to generalize such a system, and this makes rule-based NLP a labor intensive and brittle solution compared to the other NLP approaches. Here are examples of rules in a rule-based system: words ending in -ing are verbs, words ending in -er or -est are adjectives, words ending in 's are possessives, etc. Think of how many rules we would need to create by hand to make a system that could analyze and process a large volume of natural language data. Not only would the creation of rules be a mind-bogglingly difficult and tedious process, but we would also have to deal with the many errors that would occur from using such rules. We would have to create rules for rules to address all the corner cases for each and every rule.\n",
    "\n",
    "- **Traditional (or Classical) Machine Learning:** Traditional machine learning relies less on rules and more on data. It uses a statistical approach, drawing probability distributions of words based on a large annotated corpus. Humans still play a meaningful role; domain experts need to perform feature engineering to improve the machine learning model's performance. Features include capitalization, singular vs. plural, surrounding words, etc. After creating these features, you would have to train a traditional ML model to perform NLP tasks, e.g. text classification. Since traditional ML uses a statistical approach to determine when to apply certain features or rules to process language, a traditional ML-based NLP is easier to build and maintain than a rule-based system. It also generalizes better than rule-based NLP.\n",
    "\n",
    "- **Neural Networks:** Neural networks address the shortcomings of traditional machine learning. Instead of requiring humans to perform feature engineering, neural networks will \"learn\" the important features via representation learning. To perform well, these neural networks just need copious amounts of data. The amount of data required for these neural nets to perform well is substantial, but, in today's internet age, data is not too hard to acquire. You can think of neural networks as very powerful function approximators or \"rule\" creators; these rules and features are several degrees more nuanced and complex than the rules created by humans, allowing for more automated learning and more generalization of the system in processing natural language data.\n",
    "\n",
    "Of these three, the neural network-based branch of NLP, fueled by the rise of very deep neural networks (i.e., deep learning), is the most powerful and the one that has led to many of the mainstream commercial applications of NLP in recent years.\n",
    "\n",
    "In this book, we will focus mostly on neural network-based approaches to NLP, but we will also explore traditional machine learning approaches, too. The former has state-of-the-art performance in many NLP tasks, but traditional machine learning is still actively used in commercial applications. \n",
    "\n",
    "We won't focus much on rule-based NLP, but, since it has been around for decades, you will not have difficulty finding other resources on rule-based NLP. Rule-based NLP does have a room among the other two approaches but usually only to deal with edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined NLP, explored applications in vogue today, covered its history and inflection points, and clarified the different aproaches to solve NLP tasks, let's start our journey by performing the most basic tasks in NLP.\n",
    "\n",
    "We will leverage one of the most popular open-source libraries for use in commercial applications of NLP - SpaCy - to perform these tasks.\n",
    "\n",
    "Before we use SpaCy, let's discuss what these most basic NLP tasks are. As we said in the chapter introduction, these basic NLP tasks are pretty elementary, akin to teaching an elementary school child the basics of language. But, these basic NLP tasks, once combined together, help us accomplish more complex tasks, which ultimately power the major NLP applications today.\n",
    "\n",
    "Machines, like us, must walk before they run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in the chapter, we explored several NLP applications in vogue today, including the following.\n",
    "\n",
    ".NLP Applications\n",
    "* Machine translation\n",
    "* Speech recognition\n",
    "* Question answering\n",
    "* Text summarization\n",
    "* Chatbots\n",
    "* Text-to-speech and speech-to-text conversion\n",
    "* Voicebots\n",
    "* Text and audio generation\n",
    "* Sentiment analysis\n",
    "* Information extraction\n",
    "\n",
    "For machines to perform these complex applications, they need to perform several smaller, more bite-sized NLP tasks. In other words, to build successful commercial NLP applications, we must master the NLP tasks that serve as building blocks for those applications.\n",
    "\n",
    "It is important to note that modern neural network-based NLP models perform these \"tasks\" automatically through training the neural network; in other words, the neural network learns on its own how to perform some of these tasks. We, the operators, do not need to perform these tasks explicitly.\n",
    "\n",
    "These tasks are a bit outdated for this reason, but they are still relevant today both for building greater intution around how machines learn to work with natural language and for working with non-neural network based NLP models. Classical, non-neural network based NLP is still commonplace in enterprise even if it is out of favor in state of the art research today. For these reasons, it is worthwhile to learn these tasks.\n",
    "\n",
    "Without further ado, here are some of these NLP tasks.\n",
    "\n",
    ".NLP Tasks\n",
    "\n",
    "- **Tokenization:** Tokenization is the process of splitting text into minimal meaningful units such as words, punctuation marks, symbols, etc. For example, the sentence \"We live in Paris\" could be tokenized into four tokens: We, live, in, Paris. Tokenization is typically the first step of every NLP process. Tokenization is a necessary step because the machine needs to break down natural language data into the most basic elements (or tokens) so that the machine can analyze each element in context of the other elements. Otherwise, the machine would have to analyze a long garble of text or audio as if it were one singular element, making the problem intractable for the machine. Just like a beginner student of a language breaks down a sentence into smaller bits to learn and process the information word by word, a machine needs to do the same. Even with complex numerical calculations, machines break down the problem into basic elements, performing tasks such as addition, subtraction, multiplication, and division of two sets of numbers. The major advantage that the machine has is that it can do this at a pace and scale that no human can. After tokenization breaks down the text into minimal meaningful units, the machine needs to assign metadata to each unit, providing the machine more information on how to process each unit in context of other units.\n",
    "\n",
    "- **Part-of-speech Tagging:** Part-of-speech (POS) Tagging is the process to assign word types to tokens, such as noun, pronoun, verb, adverb, adjective, conjunction, preposition, interjection, etc. For \"We live in Paris,\" the parts of speech are: Pronoun, Verb, Preposition, and Noun. This part-of-speech tagging gives each token a bit more metadata, making it easier for the machine to assign relationships between each token and every other token. In the sentence, \"I kick the ball,\" \"I\" and \"ball\" are both nouns and \"kick\" is a verb. Using this metadata, we can infer that \"kick\" somehow connects \"I\" and the \"ball,\" allowing us to form a relationship among the words. This is why the parts of speech are so important. Without knowing that some words are nouns and other are verbs, etc., the machine would not be able to map the relationships among the tokens.\n",
    "\n",
    "- **Dependency Parsing:** Dependency Parsing involves labeling the relationships between individual tokens, assigning a syntactic structure to the sentence. Once the relationships are labeled, the entire sentence can be structured as a series of relationships among sets of tokens. It is easier for the machine to process text once the machine has identified the inherent structure among the text. Think of how difficult it would be for you to understand a sentence if you had all the words in the sentence presented to you out of order and you had no prior knowledge of the rules of grammar. In much the same way, until the machine performs dependency parsing, it has little to no knowledge of the structure of the text that it has converted into tokens. Once the structure is apparent, processing the text becomes a little bit easier. Dependency parsing could get tricky so the best way to understand dependency parsing is to visualize the relationships using a parse tree. AllenNLP has a great https://demo.allennlp.org/dependency-parsing/[dependency parsing demo], which we used to generate the dependency graph in Figure 1-1. This dependency graph allows us to visualize the relationships among the tokens. As you can see from the figure, \"We\" is the personal pronoun (PRP) and the nominal subject (NSUBJ) of \"live,\" which is the non-3rd person singular present verb (VBP). \"Live\" is connected to the prepositional phrase (PREP) \"in Paris.\" \"in\" is the preposition (IN), and \"Paris\" is the object of the preposition (POBJ) and is itself a singular proper noun (NNP). These relationships are very complex to model, and one of the reasons why it is very difficult to be a true master of any langauge. Most of us apply the rules of grammar on the fly, having learned language through years of experience. A machine does the same type of analysis but it has to crunch these operations one after the other at blazingly fast speeds to perform natural language processing.\n",
    "\n",
    "![Dependency parsing](images/hulp_0101.png)\n",
    "\n",
    "- **Chunking:** Chunking involves combining related tokens into a single token, creating related noun groups, related verb groups, etc. For example, \"New York City\" could be treated as a single token/chunk instead of as three separate tokens. Chunking is the process that makes this possible. Chunking is important to perform once the machine has broken the original text into tokens, identified the parts of speech, and tagged how each token is related to other tokens in the text. Chunking combines similar tokens together, making the overall process of analyzing the text a bit easier to perform. For example, instead of treating \"New,\" \"York,\" and \"City\" as three separate tokens, we can infer that they are related and group them together into a single group (or chunk). Then, we can relate the chunk to other chunks in the text. Once we've down this for the entire set of tokens, we will have a much smaller set of tokens and chunks to work with.\n",
    "\n",
    "- **Lemmatization:** Lemmatization is the process to convert words into the base forms of the words. For example, lemmatization converts horses to horse, slept to sleep, and biggest to big. Lemmatization allows the machine to simplify the text processing work it has to perform. Instead of working with a variant of the base word, it can work directly with the base word after it has performed lemmatization.\n",
    "\n",
    "- **Stemming:** Stemming is a related process to lemmatization but simpler. Stemming reduces words to their word stems. Stemming algorithms are typically rule-based. For example, the word biggest would be reduced to big, but the word slept would not be reduced at all. Stemming sometimes results in nonsensical sub-words, and we prefer lemmatiation to stemming for this reason. Lemmatization returns a word to its base or canonical form, per the dictionary. But, it is a more expensive process compared to stemming. Lemmatization requires knowing the part of speech of the word to perform well.\n",
    "\n",
    "> Note: Tokenization, part-of-speech tagging, dependency parsing, chunking, and lemmatization and stemming are tasks to process natural language for downstream NLP applications; in other words, these tasks are means to an end. Technically, the next two \"tasks\" - named entity recognition and entity linking - are not natural language tasks but rather are closer to NLP applications. Named entity recognition and entity linking can be ends themselves, rather than just means to an end. But, since they are also used for downstream NLP applications, we will include them in the \"tasks\" section here.\n",
    "\n",
    "- **Named Entity Recognition:** Named entity recognition (NER) is the process of assigning labels to known objects (or entities) such as person, organization, location, date, currency, etc. In \"We live in Paris,\" Paris would be marked as location. NER is very powerful. It allows machines to tag the most important tokens with named entity tags, and this is very important for informational retrieval applications of NLP. For example, if we want to search for former U.S. President George W. Bush in a set of documents, we would want the machine to tag all persons in all the documents using named entity recognition, and then we would search within this list of persons to find the relevant set of documents for us to investigate further.\n",
    "\n",
    "- **Entity Linking:** Entity linking is the process of disambiguating entities to an external database, linking text in one form to another. This is important both for entity resolution applications (e.g., de-duping datasets) and information retrieval applications. In the George W. Bush example, we would want to resolve all instances of George W. Bush to George W. Bush but not to George H. W. Bush, George W. Bush's father and also a former U.S. President. This resolution and linking to the correct version of President Bush is a tricky, thorny process, but one that a machine is capable of performing given all the textual context it has. Once a machine has performed entity recognition and linking, information retrieval becomes a cinch, which is one of the most commercially relevant applications of NLP today.\n",
    "\n",
    "This is just a quick and dirty overview of the most basic NLP tasks. You will want to research these tasks further; there are ample resources available online. But, for now, this is plenty of information for us to get started.\n",
    "\n",
    "Now that you know the basic NLP tasks that serve as building blocks for more ambitious NLP applications, let's use the open source NLP library SpaCy to perform some of these basic NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Programming Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the basic NLP tasks, we need to first set up our programming environment.\n",
    "\n",
    "In this book, we will use one of these easiest to use programming environments available to data scientists today - [Google’s Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb). Google Colab is a free Jupyter notebook environment that runs entirely in the cloud. In Chapter 2, we will discuss Google Colab and alternative programming environments in more detail.\n",
    "\n",
    "We will use GitHub as our coding repository.footnote:[For more on GitHub, please visit the [GitHub website](https://github.com/) and [Google Colab's instructions](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb) on how to integrate with GitHub.]\n",
    "\n",
    "If you prefer to run the code locally on your machine, we have instructions for setting up your local environment on our Github repo.\n",
    "\n",
    "With that, let’s get started with coding the basic NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy, fast.ai, and Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this book, we will use open source software libraries offered by three major companies - SpaCy, fast.ai, and Hugging Face - to perform natural language processing. These libraries are high-level, abstracting away a lot of the low-level work that we would otherwise have to do. Think of these libraries as beautiful wrappers for us to quickly apply NLP. All three libraries are performant and commercially viable, and you can pick any of the three to do your own applied work; you do not have to choose all three. That being said, it is wise to be well-versed in all three because they do have their respective strengths and weaknesses, and sometimes one will be quicker at adopting the latest advances in NLP than the others. Let us quickly introduce each of the three before we move forward with SpaCy in this chapter. In the next chapter, we will work with fast.ai and Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First released in 2015, [SpaCy](https://spacy.io/) is an open source library for NLP with blazing fast performance, leveraging both Python and Cython. Prior to SpaCy, the [Natural Language Toolkit (NLTK)](https://www.nltk.org/) was the leading NLP library among researchers, but NLTK was dated (it was initially released in 2001) and scaled poorly. SpaCy was the first modern NLP library intended for commerical audiences; it was built with scaling in production in mind. It is now one of the go-to libraries for NLP applications in enterprise, supporting over 64 languages and both TensorFlow and PyTorch.\n",
    "\n",
    "Prior to 2021, SpaCy 2.x relied on recurrent neural networks (RNNs), which we will cover later in the book, rather than the industry-leading transformer-based models. But, as of January 2021, SpaCy now supports state of the art transformer-based pipelines, too, solidifying its positioning among the major NLP libraries in use today.\n",
    "\n",
    "SpaCy's creator and parent company, [Explosion AI](http://explosion.ai/), also offers an excellent annotation platform called [Prodigy](https://prodi.gy/), which we will use in Chapter 3. Among the three libraries, SpaCy is the most mature and most extensible given all the integrations its creators have created and supported over the past six-plus years. It is the one best suited for production usage today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fast.ai](https://www.fast.ai/) (the company) released its open source library called fastai in 2018, built on top of PyTorch. fast.ai, the company, built its reputation by offering massive open online courses (MOOCs) to coders that want a more practical introduction to machine learning, and the fastai library reflects this ethos. The fastai library has high-level components that allow coders to quickly and easily produce state of the art results. At the same time, fastai has low-level components for researchers to mix and match to solve custom problems. The creators of fastai also created [ULMFiT](https://arxiv.org/abs/1801.06146), one of the first transfer learning methods in NLP, which we will use in Chapter 2. For those that would like course work and videos alongside a fast and easy-to-use library, fastai is a great option. However, it is less mature and less suited to production work than both SpaCy and Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Founded in 2016, [Hugging Face](https://huggingface.co/) is the newest comer on the block but likely the best funded and the fast-growing of the three today; the company just raised a $40 million Series B in March 2021. Hugging Face focuses exclusively on NLP and is built to help practitioners build NLP applications using state of the art transformers. Its library called transformers is built for PyTorch and TensorFlow and supports over 100 languages. In fact, it is possible to move from PyTorch and Tensorflow for development and deployment pretty seamlessly. Hugging Face also has a pipeline API for productionizing NLP models. We are most excited for the future of Hugging Face among the three libraries and highly recommend you spend sufficient time familiarizing yourself with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform NLP Tasks using SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now use SpaCy for our NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install spaCy. For more resources on how to install spaCy, please visit the [official SpaCy website](https://spacy.io/usage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't installed spaCy already, these commands will get you everything you need. If you're running them in a notebook, prefix each line with a `!` character, as we've done before.\n",
    "\n",
    "```bash\n",
    "pip install -U spacy[cuda110,transformers,lookups]==3.0.3\n",
    "pip install -U spacy-lookups-data==1.0.0\n",
    "pip install cupy-cuda110==8.5.0\n",
    "python -m spacy download en_core_web_trf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Pretrained Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy has pretrained language models for out-of-the-box use. Pretrained models are models that have been trained on lots of data already and are ready for us to perform inference with.\n",
    "\n",
    "These pretrained language models will help us solve the basic NLP tasks, but more advanced users are welcome to fine-tune the pretrained models on more specific data of their choosing. This will deliver even better performance for their specific tasks at hand.\n",
    "\n",
    "Fine-tuning is the process of taking a pretrained model and training it some more (i.e., fine-tuning the model) on a more specific corpus of text that is relevant to the domain of the user.footnote:[This operation of taking a model developed for one task and using it as a starting point for a model on a second task is known as transfer learning.] For example, if we worked in finance, we may decide to fine-tune a generic pretrained language model on financial documents to generate a finance-specific language model. This finance-specific language model would have even better performance on finance-related NLP tasks versus the generic pretrained language model.\n",
    "\n",
    "SpaCy breaks out its pretrained language models into two groups: core models and starter models. The core models are general-purpose models and will help us solve the basic NLP tasks. The starter models are base models useful for transfer learning; these models have pretrained weights which you could use to initialize and fine-tune for your own models. Think of the core models as ready-to-go models and the base models as do-it-yourself starter kits.\n",
    "\n",
    "We will use the ready-to-go core models to perform the basic NLP tasks. Let's first import the core model footnote:[A spaCy language model is not the same thing as what we generally refer to as a \"language model\" in the NLP literature. For more information on language modelling, see [[ch02]].]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy and download language model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
    
    "# Download data"
    "!aws s3 cp s3://applied-nlp-book/data/ data --recursive --no-sign-request"
    "!aws s3 cp s3://applied-nlp-book/models/ag_dataset/ models/ag_dataset --recursive --no-sign-request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s perform the first of the NLP tasks: tokenization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is where all NLP work begins; before the machine can process any of the text it sees, it must break the text into bite-size tokens. Tokenization will segment text into words, punctuation marks, etc.\n",
    "\n",
    "SpaCy automatically runs the entire NLP pipeline when you run a language model on the data (i.e., `nlp(SENTENCE)`), but to isolate just the tokenizer, we will invoke just the tokenizer using `nlp.tokenizer(SENTENCE)`.\n",
    "\n",
    "Then, we will print the length of the tokens and the individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens:  5\n",
      "The tokens: \n",
      "We\n",
      "live\n",
      "in\n",
      "Paris\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "sentence = nlp.tokenizer(\"We live in Paris.\")\n",
    "\n",
    "# Length of sentence\n",
    "print(\"The number of tokens: \", len(sentence))\n",
    "\n",
    "# Print individual words (i.e., tokens)\n",
    "print(\"The tokens: \")\n",
    "for words in sentence:\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of tokens is 5, and the individual tokens are `\"We\"`, `\"live\"`, `\"in\"`, `\"Paris\"`, `\".\"`. The period at the end of the sentence is its own token.\n",
    "\n",
    "Note that the spaCy tokenizer will treat new lines (`\"\\n\"`), tabs (`\"\\t\"`), and whitespace characters beyond a single space (`\" \"`) as tokens.\n",
    "\n",
    "Let's try the tokenizer on a slightly more complex example.\n",
    "\n",
    "We will load in publicly available Jeopardy Questions and then run the entire SpaCy language model on a few of the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Import Jeopardy Questions\n",
    "data = pd.read_csv(cwd+'/data/jeopardy_questions/jeopardy_questions.csv')\n",
    "data = pd.DataFrame(data=data)\n",
    "\n",
    "# Lowercase, strip whitespace, and view column names\n",
    "data.columns = map(lambda x: x.lower().strip(), data.columns)\n",
    "\n",
    "# Reduce size of data\n",
    "data = data[0:1000] \n",
    "\n",
    "# Tokenize Jeopardy Questions\n",
    "data[\"question_tokens\"] = data[\"question\"].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first 1,000 Jeopardy questions, we have now created tokens. In other words, you have created tokens for each and every single one of the 1,000 Jeopardy questions.\n",
    "\n",
    "To make sure everything worked right, let’s view the first question and the tokens created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first questions is:\n",
      "For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory\n"
     ]
    }
   ],
   "source": [
    "# View first question\n",
    "example_question = data.question[0]\n",
    "example_question_tokens = data.question_tokens[0]\n",
    "print(\"The first questions is:\")\n",
    "print(example_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens from the first question are:\n",
      "For\n",
      "the\n",
      "last\n",
      "8\n",
      "years\n",
      "of\n",
      "his\n",
      "life\n",
      ",\n",
      "Galileo\n",
      "was\n",
      "under\n",
      "house\n",
      "arrest\n",
      "for\n",
      "espousing\n",
      "this\n",
      "man\n",
      "'s\n",
      "theory\n"
     ]
    }
   ],
   "source": [
    "# Print individual tokens of first question\n",
    "print(\"The tokens from the first question are:\")\n",
    "for tokens in example_question_tokens:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first basic NLP task machines perform; now we can move onto the other NLP tasks. Well done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-of-speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization, machines need to tag each token with relevant metadata such as the part-of-speech of each token. This is what we will perform now.\n",
    "\n",
    "Since we applied the entire SpaCy language model to the Jeopardy questions, the tokens generated already have a lot of the meaningful attributes/metadata we care about.\n",
    "\n",
    "SpaCy uses pre-loaded statistical models to predict the part-of-speech of each token. We loaded the English language statistical model earlier using the following code: `spacy.load(\"en_core_web_sm\")`.\n",
    "\n",
    "Let's take a look at the Part-of-speech (POS) Tagging attributes for the tokens in the first question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the Part-of-speech tags for each token in the first question:\n",
      "For ADP adposition\n",
      "the DET determiner\n",
      "last ADJ adjective\n",
      "8 NUM numeral\n",
      "years NOUN noun\n",
      "of ADP adposition\n",
      "his PRON pronoun\n",
      "life NOUN noun\n",
      ", PUNCT punctuation\n",
      "Galileo PROPN proper noun\n",
      "was AUX auxiliary\n",
      "under ADP adposition\n",
      "house NOUN noun\n",
      "arrest NOUN noun\n",
      "for ADP adposition\n",
      "espousing VERB verb\n",
      "this DET determiner\n",
      "man NOUN noun\n",
      "'s PART particle\n",
      "theory NOUN noun\n"
     ]
    }
   ],
   "source": [
    "# Print Part-of-speech tags for tokens in the first question\n",
    "print(\"Here are the Part-of-speech tags for each token in the first question:\")\n",
    "for token in example_question_tokens:\n",
    "    print(token.text,token.pos_, spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first token \"For\" is marked as an adposition (e.g., in, to, during), the second token \"the\" is a determiner (e.g., a, an, the), the third token \"last\" is an adjective, the fourth token \"8\" is a numeral, the fifth token \"years\" is a noun, and so on.\n",
    "\n",
    "Figure 1-2 displays the full list of all possible POS tags, including descriptions and examples of each.footnote:[Please visit the [SpaCy POS documentation](https://spacy.io/api/annotation) for more.]\n",
    "\n",
    "![Part-of-speech Tags](images/hulp_0102.png)\n",
    "\n",
    "Now that we have used the tokenizer to create tokens for each sentence and part-of-speech tagging to tag each token with meaningful attributes, let's label each token's relationship with other tokens in the sentence. In other words, let's find the inherent structure among the tokens given the part-of-speech metadata we have generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency parsing is the process to find these relationships among the tokens. Once we have performed this step, we will be able to visualize the relationships using a dependency parsing graph.\n",
    "\n",
    "First, let's view the depenency parsing tags for each of the tokens in the first question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For prep prepositional modifier\n",
      "the det determiner\n",
      "last amod adjectival modifier\n",
      "8 nummod numeric modifier\n",
      "years pobj object of preposition\n",
      "of prep prepositional modifier\n",
      "his poss possession modifier\n",
      "life pobj object of preposition\n",
      ", punct punctuation\n",
      "Galileo nsubj nominal subject\n",
      "was ROOT None\n",
      "under prep prepositional modifier\n",
      "house compound compound\n",
      "arrest pobj object of preposition\n",
      "for prep prepositional modifier\n",
      "espousing pcomp complement of preposition\n",
      "this det determiner\n",
      "man poss possession modifier\n",
      "'s case case marking\n",
      "theory dobj direct object\n"
     ]
    }
   ],
   "source": [
    "# Print Dependency Parsing tags for tokens in the first question\n",
    "for token in example_question_tokens:\n",
    "    print(token.text,token.dep_, spacy.explain(token.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first token \"For\" is marked as a prepositional modifier, the second token \"the\" is a determiner, the third token \"last\" is an adjectival modifier, the fourth token \"8\" is a numeric modifier, the fifth token \"years\" is the object of preposition, and so on.\n",
    "\n",
    "Figures 1-3 and 1-4 list all the possible syntactic dependency tags, including descriptions and examples of each.footnote:[Please visit the [SpaCy documentation](https://spacy.io/api/annotation) for more.]\n",
    "\n",
    "![Syntactic Dependency Parsing Labels Part 1](images/hulp_0103.png)\n",
    "\n",
    "![Syntactic Dependency Parsing Labels Part 2](images/hulp_0104.png)\n",
    "\n",
    "These tags help define the relationships among the tokens; using these tags, we can understand the relationship structure among the tokens that make up the sentence.\n",
    "\n",
    "Dependency parsing is hard to unpack so let’s use spaCy’s built-in visualizer to get a better sense of the dependencies across the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"80276e7c4fd14bad8d4853ff662bb401-0\" class=\"displacy\" width=\"2330\" height=\"437.0\" direction=\"ltr\" style=\"max-width: none; height: 437.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">For</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">last</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">8</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">years</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">his</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">life,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">Galileo</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">under</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1370\">house</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1370\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1490\">arrest</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1490\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1610\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1610\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1730\">espousing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1730\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1850\">this</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1850\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1970\">man</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1970\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2090\">'s</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2090\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2210\">theory</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2210\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-0\" stroke-width=\"2px\" d=\"M70,302.0 C70,2.0 1130.0,2.0 1130.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,304.0 L62,292.0 78,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-1\" stroke-width=\"2px\" d=\"M190,302.0 C190,122.0 520.0,122.0 520.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M190,304.0 L182,292.0 198,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-2\" stroke-width=\"2px\" d=\"M310,302.0 C310,182.0 515.0,182.0 515.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310,304.0 L302,292.0 318,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-3\" stroke-width=\"2px\" d=\"M430,302.0 C430,242.0 510.0,242.0 510.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M430,304.0 L422,292.0 438,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-4\" stroke-width=\"2px\" d=\"M70,302.0 C70,62.0 525.0,62.0 525.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M525.0,304.0 L533.0,292.0 517.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-5\" stroke-width=\"2px\" d=\"M550,302.0 C550,242.0 630.0,242.0 630.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M630.0,304.0 L638.0,292.0 622.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-6\" stroke-width=\"2px\" d=\"M790,302.0 C790,242.0 870.0,242.0 870.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,304.0 L782,292.0 798,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-7\" stroke-width=\"2px\" d=\"M670,302.0 C670,182.0 875.0,182.0 875.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M875.0,304.0 L883.0,292.0 867.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-8\" stroke-width=\"2px\" d=\"M1030,302.0 C1030,242.0 1110.0,242.0 1110.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1030,304.0 L1022,292.0 1038,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-9\" stroke-width=\"2px\" d=\"M1150,302.0 C1150,242.0 1230.0,242.0 1230.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1230.0,304.0 L1238.0,292.0 1222.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-10\" stroke-width=\"2px\" d=\"M1390,302.0 C1390,242.0 1470.0,242.0 1470.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1390,304.0 L1382,292.0 1398,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-11\" stroke-width=\"2px\" d=\"M1270,302.0 C1270,182.0 1475.0,182.0 1475.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1475.0,304.0 L1483.0,292.0 1467.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-12\" stroke-width=\"2px\" d=\"M1510,302.0 C1510,242.0 1590.0,242.0 1590.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1590.0,304.0 L1598.0,292.0 1582.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-13\" stroke-width=\"2px\" d=\"M1630,302.0 C1630,242.0 1710.0,242.0 1710.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1710.0,304.0 L1718.0,292.0 1702.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-14\" stroke-width=\"2px\" d=\"M1870,302.0 C1870,242.0 1950.0,242.0 1950.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1870,304.0 L1862,292.0 1878,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-15\" stroke-width=\"2px\" d=\"M1990,302.0 C1990,182.0 2195.0,182.0 2195.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1990,304.0 L1982,292.0 1998,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-16\" stroke-width=\"2px\" d=\"M1990,302.0 C1990,242.0 2070.0,242.0 2070.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2070.0,304.0 L2078.0,292.0 2062.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-80276e7c4fd14bad8d4853ff662bb401-0-17\" stroke-width=\"2px\" d=\"M1750,302.0 C1750,62.0 2205.0,62.0 2205.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-80276e7c4fd14bad8d4853ff662bb401-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2205.0,304.0 L2213.0,292.0 2197.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the dependency parse\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(example_question_tokens, style='dep',\n",
    "                jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1-5 displays the first part of the sentence parsed.\n",
    "\n",
    "![Dependency Parsing Example - Part 1](images/hulp_0105.png)\n",
    "\n",
    "Notice the importance of \"For\" and \"years\" in the prepositional phrase -- multiple tokens map to these two.\n",
    "\n",
    "Figure 1-6 displays the second part of the sentence parsed.\n",
    "\n",
    "![Dependency Parsing Example - Part 2](images/hulp_0106.png)\n",
    "\n",
    "The token \"was\" connects to the nominal subject \"Galileo\" and two prepositional phrases: \"under house arrest\" and \"for espousing this man's theory\".\n",
    "\n",
    "These figures show how certain tokens can be grouped together and how the groups of tokens are related to one another. This is an essential step in natural language processing. First, the machine breaks the sentence apart into tokens. Then it assigns metadata to each token (e.g., part of speech), and then it connects the tokens based on their relationship to one another.\n",
    "\n",
    "Let's move on to chunking, which is another form of grouping of related tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s perform chunking on the following sentence: \"My parents live in New York City\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My\n",
      "parents\n",
      "live\n",
      "in\n",
      "New\n",
      "York\n",
      "City\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Print tokens for example sentence without chunking\n",
    "for token in nlp(\"My parents live in New York City.\"):\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking combines related tokens into a single token.\n",
    "\n",
    "With chunking, the spaCy language model will identify \"My parents\" and \"New York City\" as noun chunks much like humans would when parsing a sentence in their head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My parents\n",
      "New York City\n"
     ]
    }
   ],
   "source": [
    "# Print chunks for example sentence\n",
    "for chunk in nlp(\"My parents live in New York City.\").noun_chunks:\n",
    "      print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By grouping related tokens into chunks, the machine will have an easier time processing the sentence. Instead of viewing each token in isolation, the machine now recognizes that certain tokens are related to others, a necessary step in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s go a step further and perform lemmatization. If you recall, lemmatization is the process to convert words into the base (or canonical) forms of the words. For example, horses to horse, slept to sleep, and biggest to big. Just like part-of-speech tagging, dependency parsing, and chunking, lemmatization helps the machine \"process\" the tokens. With lemmatization, the machine is able to simplify the tokens by converting some of the tokens into their most basic forms.\n",
    "\n",
    "Stemming is a related concept, but stemming is simpler. Stemming reduces words to their word stems, often using a rule-based approach.\n",
    "\n",
    "Lemmatization is a more difficult process but generally results in better outputs; stemming sometimes creates outputs that are non-sensical (non-words). In fact, spaCy does not even support stemming; it supports only lemmatization.\n",
    "\n",
    "We will create a DataFrame to store and view the original and lemmatized versions of tokens side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>last</td>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>years</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>his</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>life</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Galileo</td>\n",
       "      <td>Galileo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>under</td>\n",
       "      <td>under</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>house</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>arrest</td>\n",
       "      <td>arrest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>espousing</td>\n",
       "      <td>espouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>man</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>'s</td>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>theory</td>\n",
       "      <td>theory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     original lemmatized\n",
       "0         For        for\n",
       "1         the        the\n",
       "2        last       last\n",
       "3           8          8\n",
       "4       years       year\n",
       "5          of         of\n",
       "6         his        his\n",
       "7        life       life\n",
       "8           ,          ,\n",
       "9     Galileo    Galileo\n",
       "10        was         be\n",
       "11      under      under\n",
       "12      house      house\n",
       "13     arrest     arrest\n",
       "14        for        for\n",
       "15  espousing    espouse\n",
       "16       this       this\n",
       "17        man        man\n",
       "18         's         's\n",
       "19     theory     theory"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print Lemmatization for tokens in the first question\n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"original\",\"lemmatized\"])\n",
    "i = 0\n",
    "for token in example_question_tokens:\n",
    "    lemmatization.loc[i,\"original\"] = token.text\n",
    "    lemmatization.loc[i,\"lemmatized\"] = token.lemma_\n",
    "    i = i+1\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, words such as \"years\", \"was\", and \"espousing\" are lemmatized to their base forms. The other tokens are already their base forms, so the lemmatized output is the same as the original. Lemmatization simplifies tokens into their simplest forms, where possible, to simplify the process for the machine to parse sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When combined together, everything we've done so far - tokenization, part-of-speech tagging, dependency parsing, chunking, and lemmatization - makes it possible for machines to perform more complex NLP tasks.\n",
    "\n",
    "One example of a complex NLP task is named entity recognition (NER). Named entity recognition parses notable entities in natural language and labels them with their appropriate class label. For example, NER labels names of people with the label \"Person\" and names of cities with the label \"Location.\" \n",
    "\n",
    "NER is possible only because the machine is able to perform text classification using the metadata generated by the earlier NLP tasks we've covered. Without the metadata from the earlier NLP tasks, the machine would have a very difficult time performing NER because it would not have enough features to classify names of people as \"Person,\" names of cities as \"Location,\" etc.\n",
    "\n",
    "NER is a valuable NLP task because many organizations need to process lots and lots of documents in volume, and the simple act of labeling notable entities with the appropriate class label is a meaningful first step in analyzing the textual information, particularly for information retrieval tasks (e.g., finding information that you need as quickly as possible).\n",
    "\n",
    "These documents include contracts, leases, real estate purchase agreements, financial reports, news articles, etc. Before named entity recogniton, humans would have had to label such entities by hand (at many companies, they still do). Now, named entity recognition (also known as \"NER\") provides an algorithmic way to perform this task.\n",
    "\n",
    "SpaCy's NER model is able to label many types of notable entities (\"real-world objects\"). Figure 1-7 displays the current set of entity types the spaCy model is able to recognize.\n",
    "\n",
    "![spaCy NER Entity Types](images/hulp_0107.png)\n",
    "\n",
    "It's very important to note that NER is, at its very core, a classification model. Using the context of tokens around the token of interest, the NER model predicts the entity type of the token of interest. NER is a statistical model, and the corpus of data the model has trained on matters a lot. For better performance, developers of these models in enterprise will finetune the base NER models on their particular corpus of documents to achieve better performance versus the base NER model.\n",
    "\n",
    "Let's try the spaCy NER model. We will perform NER on the first sentence describing George Washington, the first president of the United States, from his [Wikipedia article](https://en.wikipedia.org/wiki/George_Washington).\n",
    "\n",
    "Here's the sentence: George Washington was an American political leader, military general, statesman, and Founding Father who served as the first president of the United States from 1789 to 1797.\n",
    "\n",
    "As you can see above, there are several real-world objects to recognize here including George Washington and the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Washington was an American political leader, military general, statesman, and Founding Father who served as the first president of the United States from 1789 to 1797.\n",
      "\n",
      "Text Start End Label\n",
      "George Washington 0 17 PERSON\n",
      "American 25 33 NORP\n",
      "first 119 124 ORDINAL\n",
      "the United States 138 155 GPE\n",
      "1789 to 1797 161 173 DATE\n"
     ]
    }
   ],
   "source": [
    "# Print NER results\n",
    "example_sentence = \"George Washington was an American political leader, \\\n",
    "military general, statesman, and Founding Father who served as the \\\n",
    "first president of the United States from 1789 to 1797.\\n\"\n",
    "\n",
    "print(example_sentence)\n",
    "\n",
    "print(\"Text Start End Label\")\n",
    "doc = nlp(example_sentence)\n",
    "for token in doc.ents:\n",
    "    print(token.text, token.start_char, token.end_char, token.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are four elements to the output. First, the text that comprises the entity; note that the text could be a single token or a set of token that makes up the entire entity. Second, the start position of the text in the sentence. Third, the end position of the text in the sentence. Fourth, the label of the entity.\n",
    "\n",
    "To make the value of NER even more apparent, let’s use spaCy’s built-in visualizer to visualize this sentence with the releveant entity labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    George Washington\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was an \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " political leader, military general, statesman, and Founding Father who served as the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " president of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " from \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1789 to 1797\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize NER results\n",
    "displacy.render(doc, style='ent', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in Figure 1-8, the spaCy NER model does a great job labeling the entities. \"George Washington\" is a person and the text starts at index 0 and ends at index 17. His nationality is \"American\". \"first\" is labeled as an ordinal number, \"the United States\" is a geopolitical entity, and \"1789 to 1797\" is a date.\n",
    "\n",
    "![Visualize NER Results](images/hulp_0108.png)\n",
    "\n",
    "The sentence is beautifully rendered with color-coded labels based on the entity type. This is a powerful and meaningful NLP task; you could see how doing this machine-driven labeling at scale without humans could add a lot of value to enterprises that work with a lot of textual data. Of course, to train such a model in the first place, you do need to have a lot of humans that annotate textual data. And you may need humans in the loop to deal with edge cases in production. You are never really human-free, but perhaps you could ultimately get to a process that is mostly human-free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another complex yet very useful NLP task in enterprise is named entity linking (NEL). Named entity linking resolves a textual entity to a unique identifier in a knowledge base. In other words, NEL resolves the entity in your source text to a canonical version in a knowledge database. Let’s try to link all entities that are named persons to Google’s Knowledge Graph. We will make a Google Knowledge Graph API call to perform this named entity linking.footnote:[You will need your own [Google Knowledge Graph API key](https://developers.google.com/knowledge-graph) to perform this API call on your own machine. We will perform this using our own API key for illustrative purposes.]\n",
    "\n",
    "Here is the function to perform this API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "\n",
    "# Define Google Knowledge Graph API Result function\n",
    "def returnGraphResult(query, key, entityType):\n",
    "    if entityType==\"PERSON\":\n",
    "        google = f\"https://kgsearch.googleapis.com/v1/entities:search\\\n",
    "         ?query={query}&key={key}\"\n",
    "        resp = requests.get(google)\n",
    "        url = resp.json()['itemListElement'][0]['result']\\\n",
    "         ['detailedDescription']['url']\n",
    "        description = resp.json()['itemListElement'][0]['result']\\\n",
    "         ['detailedDescription']['articleBody']\n",
    "        return url, description\n",
    "    else:\n",
    "        return \"no_match\", \"no_match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s perform entity linking on our George Washington example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor token in doc.ents:\\n    url, description = returnGraphResult(token.text, key, token.label_)\\n    print(token.text, token.label_, url, description)\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print Wikipedia descriptions and urls for entities\n",
    "# You can un-comment this and run the code after you obtain your own Google Knowledge Graph API key\n",
    "'''\n",
    "for token in doc.ents:\n",
    "    url, description = returnGraphResult(token.text, key, token.label_)\n",
    "    print(token.text, token.label_, url, description)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the output.\n",
    "\n",
    "- George Washington:: PERSON https://en.wikipedia.org/wiki/George_Washington George Washington was an American political leader, military general, statesman, and Founding Father, who also served as the first President of the United States from 1789 to 1797. \n",
    "- American:: NORP no_match no_match\n",
    "- first:: ORDINAL no_match no_match\n",
    "- the United States:: GPE no_match no_match\n",
    "- 1789 to 1797:: DATE no_match no_match\n",
    "\n",
    "As you can see, George Washing is a PERSON and is linked successfully to the \"George Washington\" Wikipedia url and description. The rest are not of entity type PERSON and are not linked. If desired, we could link the other named entities, such as the United States, to relevant Wikipedia articles, too.\n",
    "\n",
    "Named entity linking has many use cases in enterprise, especially since the need to link information to a taxonomy comes up over and over again (e.g., linking stock tickers, pharmaceutical drugs, publicly traded companies, consumer products, etc. to canonical versions in a taxonomy or knowledge base)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we defined NLP and covered its origins, including some of the commercial applications that are popular in enterprise today. Then, we defined some basic NLP tasks and performed them using a very performant NLP library known as SpaCy. You should spend more time using SpaCy, including reviewing documentation that is available online, to hone what you have learned in this chapter.\n",
    "\n",
    "While the tasks we performed are very basic, when combined together, NLP tasks such as tokenization, part-of-speech tagging, dependency parsing, chunking, and lemmatization make it possible for machines to perform even more complex NLP tasks such as named entity recognition and entity linking. We hope our walkthrough of these tasks helped you build some intuition on just how machines are able to unpack and process natural language, demystifying some of the space.\n",
    "\n",
    "Today, most complex NLP applications do not require practitioners to perform these tasks manually; rather neural networks learn to perform these \"tasks\" on their own. In the next chapter, we will dive into some of the state of the art approaches using the Transformer architecture and large, pretrained language models from fastai and Hugging Face to show just how easy it is to get up and running with NLP today. Later in the book, we will return to the basics (which we just teased you with briefly in this chapter) and help you build more of your foundational knowledge of NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:applied_nlp_base_mac] *",
   "language": "python",
   "name": "conda-env-applied_nlp_base_mac-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
